{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "sys.version_info(major=3, minor=7, micro=6, releaselevel='final', serial=0)\n",
      "matplotlib 3.1.0\n",
      "numpy 1.16.4\n",
      "pandas 0.24.2\n",
      "sklearn 0.21.3\n",
      "tensorflow 2.1.0\n",
      "tensorflow_core.python.keras.api._v2.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2812, 4) (2812,)\n",
      "(938, 4) (938,)\n",
      "(1250, 4) (1250,)\n"
     ]
    }
   ],
   "source": [
    "# read data from file\n",
    "df = pd.read_csv(\"./data/AdvectionDiffusion/noise_00.csv\")\n",
    "x = df[['x', 'y', 'z', 't']].values\n",
    "y = df['u'].values\n",
    "u_t = df['u_t'].values\n",
    "u_x = df['u_x'].values\n",
    "u_y = df['u_y'].values\n",
    "u_z = df['u_z'].values\n",
    "u_xx = df['u_xx'].values\n",
    "u_yy = df['u_yy'].values\n",
    "u_zz = df['u_zz'].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(x, y, random_state=7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_all, y_train_all, random_state=11)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hidden_layers = 1, layer_size = 30, learning_rate = 3e-5):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(layer_size, activation=\"sigmoid\",\n",
    "                                input_shape=x_train.shape[1:]))\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(keras.layers.Dense(layer_size,\n",
    "                                    activation = \"sigmoid\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(loss = 'mse', optimizer = optimizer)\n",
    "    return model\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=5, min_delta=1e-10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct the machine learning model\n",
    "def s(input_x, input_y, input_z, input_t):\n",
    "    i = 0\n",
    "    for w, b in zip(kernel_tensor, bias_tensor):\n",
    "        if i == 0:\n",
    "            input_tensor = input_x*w[0,:] + input_y*w[1,:] + input_z*w[2,:] + input_t*w[3,:] + b\n",
    "            input_tensor = tf.nn.sigmoid(input_tensor)\n",
    "        elif i < len(kernel_tensor)-1:\n",
    "            output_tensor = input_tensor@w + b\n",
    "            output = tf.nn.sigmoid(output_tensor)\n",
    "            input_tensor = output\n",
    "        else:\n",
    "            output_tensor = input_tensor@w + b\n",
    "            output = output_tensor\n",
    "        i += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = x\n",
    "input_x = tf.Variable(input_array[:, 0].reshape(5000, 1))\n",
    "input_y = tf.Variable(input_array[:, 1].reshape(5000, 1))\n",
    "input_z = tf.Variable(input_array[:, 2].reshape(5000, 1))\n",
    "input_t = tf.Variable(input_array[:, 3].reshape(5000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "layers = [i for i in range(1, 11)]\n",
    "dx = []\n",
    "dy = []\n",
    "dz = []\n",
    "dt = []\n",
    "dxx = []\n",
    "dyy = []\n",
    "dzz = []\n",
    "print(\"layers: \", layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2812 samples, validate on 938 samples\n",
      "Epoch 1/100\n",
      "2812/2812 [==============================] - 0s 87us/sample - loss: 0.0876 - val_loss: 0.0753\n",
      "Epoch 2/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0849 - val_loss: 0.0730\n",
      "Epoch 3/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0825 - val_loss: 0.0708\n",
      "Epoch 4/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0803 - val_loss: 0.0689\n",
      "Epoch 5/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0782 - val_loss: 0.0671\n",
      "Epoch 6/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0764 - val_loss: 0.0655\n",
      "Epoch 7/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0747 - val_loss: 0.0641\n",
      "Epoch 8/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0732 - val_loss: 0.0627\n",
      "Epoch 9/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0717 - val_loss: 0.0615\n",
      "Epoch 10/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0705 - val_loss: 0.0604\n",
      "Epoch 11/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0693 - val_loss: 0.0594\n",
      "Epoch 12/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0682 - val_loss: 0.0585\n",
      "Epoch 13/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0672 - val_loss: 0.0577\n",
      "Epoch 14/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0663 - val_loss: 0.0570\n",
      "Epoch 15/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0655 - val_loss: 0.0563\n",
      "Epoch 16/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0648 - val_loss: 0.0557\n",
      "Epoch 17/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0641 - val_loss: 0.0551\n",
      "Epoch 18/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0634 - val_loss: 0.0546\n",
      "Epoch 19/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0629 - val_loss: 0.0541\n",
      "Epoch 20/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0624 - val_loss: 0.0537\n",
      "Epoch 21/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0619 - val_loss: 0.0534\n",
      "Epoch 22/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0614 - val_loss: 0.0530\n",
      "Epoch 23/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0610 - val_loss: 0.0527\n",
      "Epoch 24/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0607 - val_loss: 0.0524\n",
      "Epoch 25/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0603 - val_loss: 0.0522\n",
      "Epoch 26/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0600 - val_loss: 0.0520\n",
      "Epoch 27/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0598 - val_loss: 0.0518\n",
      "Epoch 28/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0595 - val_loss: 0.0516\n",
      "Epoch 29/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0593 - val_loss: 0.0514\n",
      "Epoch 30/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0591 - val_loss: 0.0513\n",
      "Epoch 31/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0589 - val_loss: 0.0511\n",
      "Epoch 32/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0587 - val_loss: 0.0510\n",
      "Epoch 33/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0585 - val_loss: 0.0509\n",
      "Epoch 34/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0584 - val_loss: 0.0508\n",
      "Epoch 35/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0582 - val_loss: 0.0507\n",
      "Epoch 36/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0581 - val_loss: 0.0506\n",
      "Epoch 37/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0580 - val_loss: 0.0506\n",
      "Epoch 38/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0579 - val_loss: 0.0505\n",
      "Epoch 39/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0578 - val_loss: 0.0504\n",
      "Epoch 40/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0577 - val_loss: 0.0504\n",
      "Epoch 41/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0576 - val_loss: 0.0503\n",
      "Epoch 42/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0576 - val_loss: 0.0503\n",
      "Epoch 43/100\n",
      "2812/2812 [==============================] - 0s 26us/sample - loss: 0.0575 - val_loss: 0.0503\n",
      "Epoch 44/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0574 - val_loss: 0.0502\n",
      "Epoch 45/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0574 - val_loss: 0.0502\n",
      "Epoch 46/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0573 - val_loss: 0.0502\n",
      "Epoch 47/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0573 - val_loss: 0.0502\n",
      "Epoch 48/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0572 - val_loss: 0.0501\n",
      "Epoch 49/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0572 - val_loss: 0.0501\n",
      "Epoch 50/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0572 - val_loss: 0.0501\n",
      "Epoch 51/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0571 - val_loss: 0.0501\n",
      "Epoch 52/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0571 - val_loss: 0.0501\n",
      "Epoch 53/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0571 - val_loss: 0.0501\n",
      "Epoch 54/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0570 - val_loss: 0.0501\n",
      "Epoch 55/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0570 - val_loss: 0.0501\n",
      "Epoch 56/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0570 - val_loss: 0.0501\n",
      "Epoch 57/100\n",
      "2812/2812 [==============================] - 0s 26us/sample - loss: 0.0570 - val_loss: 0.0501\n",
      "Epoch 58/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0570 - val_loss: 0.0501\n",
      "Epoch 59/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0569 - val_loss: 0.0501\n",
      "Epoch 60/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0569 - val_loss: 0.0501\n",
      "Epoch 61/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0569 - val_loss: 0.0501\n",
      "Epoch 62/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0569 - val_loss: 0.0501\n",
      "Epoch 63/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0569 - val_loss: 0.0501\n",
      "Epoch 64/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0569 - val_loss: 0.0501\n",
      "Epoch 65/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0568 - val_loss: 0.0501\n",
      "Epoch 66/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0568 - val_loss: 0.0501\n",
      "Epoch 67/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0568 - val_loss: 0.0501\n",
      "Epoch 68/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0568 - val_loss: 0.0501\n",
      "Epoch 69/100\n",
      "2812/2812 [==============================] - 0s 27us/sample - loss: 0.0568 - val_loss: 0.0501\n",
      "Train on 2812 samples, validate on 938 samples\n",
      "Epoch 1/100\n",
      "2812/2812 [==============================] - 0s 95us/sample - loss: 0.7304 - val_loss: 0.6647\n",
      "Epoch 2/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.6607 - val_loss: 0.5999\n",
      "Epoch 3/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.5983 - val_loss: 0.5420\n",
      "Epoch 4/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.5424 - val_loss: 0.4902\n",
      "Epoch 5/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.4923 - val_loss: 0.4438\n",
      "Epoch 6/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.4475 - val_loss: 0.4023\n",
      "Epoch 7/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.4073 - val_loss: 0.3651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.3713 - val_loss: 0.3319\n",
      "Epoch 9/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.3391 - val_loss: 0.3021\n",
      "Epoch 10/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.3101 - val_loss: 0.2755\n",
      "Epoch 11/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.2842 - val_loss: 0.2517\n",
      "Epoch 12/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.2610 - val_loss: 0.2304\n",
      "Epoch 13/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.2402 - val_loss: 0.2113\n",
      "Epoch 14/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.2216 - val_loss: 0.1943\n",
      "Epoch 15/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.2049 - val_loss: 0.1790\n",
      "Epoch 16/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.1899 - val_loss: 0.1653\n",
      "Epoch 17/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.1764 - val_loss: 0.1531\n",
      "Epoch 18/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.1644 - val_loss: 0.1422\n",
      "Epoch 19/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.1536 - val_loss: 0.1324\n",
      "Epoch 20/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.1440 - val_loss: 0.1237\n",
      "Epoch 21/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.1353 - val_loss: 0.1159\n",
      "Epoch 22/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.1275 - val_loss: 0.1089\n",
      "Epoch 23/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.1205 - val_loss: 0.1027\n",
      "Epoch 24/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.1143 - val_loss: 0.0971\n",
      "Epoch 25/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.1087 - val_loss: 0.0921\n",
      "Epoch 26/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.1037 - val_loss: 0.0877\n",
      "Epoch 27/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0992 - val_loss: 0.0837\n",
      "Epoch 28/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0951 - val_loss: 0.0801\n",
      "Epoch 29/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0915 - val_loss: 0.0770\n",
      "Epoch 30/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0883 - val_loss: 0.0741\n",
      "Epoch 31/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0853 - val_loss: 0.0716\n",
      "Epoch 32/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0827 - val_loss: 0.0693\n",
      "Epoch 33/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0804 - val_loss: 0.0673\n",
      "Epoch 34/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0783 - val_loss: 0.0655\n",
      "Epoch 35/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0764 - val_loss: 0.0639\n",
      "Epoch 36/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0747 - val_loss: 0.0625\n",
      "Epoch 37/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0732 - val_loss: 0.0613\n",
      "Epoch 38/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0719 - val_loss: 0.0601\n",
      "Epoch 39/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0706 - val_loss: 0.0591\n",
      "Epoch 40/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0696 - val_loss: 0.0582\n",
      "Epoch 41/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0686 - val_loss: 0.0574\n",
      "Epoch 42/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0677 - val_loss: 0.0567\n",
      "Epoch 43/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0669 - val_loss: 0.0561\n",
      "Epoch 44/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0662 - val_loss: 0.0555\n",
      "Epoch 45/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0656 - val_loss: 0.0551\n",
      "Epoch 46/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0650 - val_loss: 0.0546\n",
      "Epoch 47/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0645 - val_loss: 0.0542\n",
      "Epoch 48/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0640 - val_loss: 0.0539\n",
      "Epoch 49/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0636 - val_loss: 0.0536\n",
      "Epoch 50/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0633 - val_loss: 0.0533\n",
      "Epoch 51/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0629 - val_loss: 0.0531\n",
      "Epoch 52/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0626 - val_loss: 0.0529\n",
      "Epoch 53/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0624 - val_loss: 0.0527\n",
      "Epoch 54/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0621 - val_loss: 0.0525\n",
      "Epoch 55/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0619 - val_loss: 0.0524\n",
      "Epoch 56/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0617 - val_loss: 0.0523\n",
      "Epoch 57/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0616 - val_loss: 0.0522\n",
      "Epoch 58/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0614 - val_loss: 0.0521\n",
      "Epoch 59/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0613 - val_loss: 0.0520\n",
      "Epoch 60/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0612 - val_loss: 0.0519\n",
      "Epoch 61/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0610 - val_loss: 0.0519\n",
      "Epoch 62/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0609 - val_loss: 0.0518\n",
      "Epoch 63/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0609 - val_loss: 0.0518\n",
      "Epoch 64/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0608 - val_loss: 0.0517\n",
      "Epoch 65/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0607 - val_loss: 0.0517\n",
      "Epoch 66/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0606 - val_loss: 0.0517\n",
      "Epoch 67/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0606 - val_loss: 0.0516\n",
      "Epoch 68/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0605 - val_loss: 0.0516\n",
      "Epoch 69/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0605 - val_loss: 0.0516\n",
      "Epoch 70/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0604 - val_loss: 0.0516\n",
      "Epoch 71/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0604 - val_loss: 0.0516\n",
      "Epoch 72/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0604 - val_loss: 0.0516\n",
      "Epoch 73/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0603 - val_loss: 0.0516\n",
      "Epoch 74/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0603 - val_loss: 0.0516\n",
      "Epoch 75/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0603 - val_loss: 0.0516\n",
      "Epoch 76/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0603 - val_loss: 0.0516\n",
      "Epoch 77/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0602 - val_loss: 0.0516\n",
      "Epoch 78/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0602 - val_loss: 0.0516\n",
      "Epoch 79/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0602 - val_loss: 0.0516\n",
      "Epoch 80/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0602 - val_loss: 0.0516\n",
      "Epoch 81/100\n",
      "2812/2812 [==============================] - 0s 28us/sample - loss: 0.0602 - val_loss: 0.0516\n",
      "Epoch 82/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0602 - val_loss: 0.0516\n",
      "Train on 2812 samples, validate on 938 samples\n",
      "Epoch 1/100\n",
      "2812/2812 [==============================] - 0s 97us/sample - loss: 0.1093 - val_loss: 0.0933\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.1041 - val_loss: 0.0887\n",
      "Epoch 3/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0995 - val_loss: 0.0846\n",
      "Epoch 4/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0953 - val_loss: 0.0809\n",
      "Epoch 5/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0916 - val_loss: 0.0776\n",
      "Epoch 6/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0882 - val_loss: 0.0746\n",
      "Epoch 7/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0851 - val_loss: 0.0719\n",
      "Epoch 8/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0824 - val_loss: 0.0695\n",
      "Epoch 9/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0799 - val_loss: 0.0674\n",
      "Epoch 10/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0777 - val_loss: 0.0654\n",
      "Epoch 11/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0756 - val_loss: 0.0637\n",
      "Epoch 12/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0738 - val_loss: 0.0621\n",
      "Epoch 13/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0722 - val_loss: 0.0607\n",
      "Epoch 14/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0707 - val_loss: 0.0595\n",
      "Epoch 15/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0694 - val_loss: 0.0584\n",
      "Epoch 16/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0682 - val_loss: 0.0574\n",
      "Epoch 17/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0671 - val_loss: 0.0565\n",
      "Epoch 18/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0661 - val_loss: 0.0557\n",
      "Epoch 19/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0652 - val_loss: 0.0549\n",
      "Epoch 20/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0645 - val_loss: 0.0543\n",
      "Epoch 21/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0637 - val_loss: 0.0537\n",
      "Epoch 22/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0631 - val_loss: 0.0532\n",
      "Epoch 23/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0625 - val_loss: 0.0528\n",
      "Epoch 24/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0620 - val_loss: 0.0524\n",
      "Epoch 25/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0615 - val_loss: 0.0520\n",
      "Epoch 26/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0611 - val_loss: 0.0517\n",
      "Epoch 27/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0607 - val_loss: 0.0514\n",
      "Epoch 28/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0604 - val_loss: 0.0511\n",
      "Epoch 29/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0601 - val_loss: 0.0509\n",
      "Epoch 30/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0598 - val_loss: 0.0507\n",
      "Epoch 31/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0595 - val_loss: 0.0505\n",
      "Epoch 32/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0593 - val_loss: 0.0504\n",
      "Epoch 33/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0591 - val_loss: 0.0502\n",
      "Epoch 34/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0589 - val_loss: 0.0501\n",
      "Epoch 35/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0587 - val_loss: 0.0500\n",
      "Epoch 36/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0586 - val_loss: 0.0499\n",
      "Epoch 37/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0585 - val_loss: 0.0498\n",
      "Epoch 38/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0583 - val_loss: 0.0498\n",
      "Epoch 39/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0582 - val_loss: 0.0497\n",
      "Epoch 40/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0581 - val_loss: 0.0496\n",
      "Epoch 41/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0580 - val_loss: 0.0496\n",
      "Epoch 42/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0579 - val_loss: 0.0496\n",
      "Epoch 43/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0579 - val_loss: 0.0495\n",
      "Epoch 44/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0578 - val_loss: 0.0495\n",
      "Epoch 45/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0578 - val_loss: 0.0495\n",
      "Epoch 46/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0577 - val_loss: 0.0494\n",
      "Epoch 47/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0577 - val_loss: 0.0494\n",
      "Epoch 48/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0576 - val_loss: 0.0494\n",
      "Epoch 49/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0576 - val_loss: 0.0494\n",
      "Epoch 50/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0575 - val_loss: 0.0494\n",
      "Epoch 51/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0575 - val_loss: 0.0494\n",
      "Epoch 52/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0575 - val_loss: 0.0494\n",
      "Epoch 53/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0574 - val_loss: 0.0494\n",
      "Epoch 54/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0574 - val_loss: 0.0494\n",
      "Epoch 55/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0574 - val_loss: 0.0494\n",
      "Epoch 56/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0574 - val_loss: 0.0494\n",
      "Epoch 57/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0574 - val_loss: 0.0494\n",
      "Epoch 58/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0574 - val_loss: 0.0494\n",
      "Epoch 59/100\n",
      "2812/2812 [==============================] - 0s 29us/sample - loss: 0.0573 - val_loss: 0.0494\n",
      "Epoch 60/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0573 - val_loss: 0.0494\n",
      "Epoch 61/100\n",
      "2812/2812 [==============================] - 0s 30us/sample - loss: 0.0573 - val_loss: 0.0494\n",
      "Train on 2812 samples, validate on 938 samples\n",
      "Epoch 1/100\n",
      "2812/2812 [==============================] - 0s 107us/sample - loss: 0.0590 - val_loss: 0.0522\n",
      "Epoch 2/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0588 - val_loss: 0.0520\n",
      "Epoch 3/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0586 - val_loss: 0.0518\n",
      "Epoch 4/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0585 - val_loss: 0.0516\n",
      "Epoch 5/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0583 - val_loss: 0.0514\n",
      "Epoch 6/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0582 - val_loss: 0.0513\n",
      "Epoch 7/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0581 - val_loss: 0.0511\n",
      "Epoch 8/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0580 - val_loss: 0.0510\n",
      "Epoch 9/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0579 - val_loss: 0.0509\n",
      "Epoch 10/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0578 - val_loss: 0.0507\n",
      "Epoch 11/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0577 - val_loss: 0.0506\n",
      "Epoch 12/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0577 - val_loss: 0.0505\n",
      "Epoch 13/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0576 - val_loss: 0.0505\n",
      "Epoch 14/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0576 - val_loss: 0.0504\n",
      "Epoch 15/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0575 - val_loss: 0.0503\n",
      "Epoch 16/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0575 - val_loss: 0.0502\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0574 - val_loss: 0.0502\n",
      "Epoch 18/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0574 - val_loss: 0.0501\n",
      "Epoch 19/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0573 - val_loss: 0.0501\n",
      "Epoch 20/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0573 - val_loss: 0.0500\n",
      "Epoch 21/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0573 - val_loss: 0.0500\n",
      "Epoch 22/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0573 - val_loss: 0.0499\n",
      "Epoch 23/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0572 - val_loss: 0.0499\n",
      "Epoch 24/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0572 - val_loss: 0.0498\n",
      "Epoch 25/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0572 - val_loss: 0.0498\n",
      "Epoch 26/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0572 - val_loss: 0.0498\n",
      "Epoch 27/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0572 - val_loss: 0.0497\n",
      "Epoch 28/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0572 - val_loss: 0.0497\n",
      "Epoch 29/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0572 - val_loss: 0.0497\n",
      "Epoch 30/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0571 - val_loss: 0.0497\n",
      "Epoch 31/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0571 - val_loss: 0.0497\n",
      "Epoch 32/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0571 - val_loss: 0.0496\n",
      "Epoch 33/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0571 - val_loss: 0.0496\n",
      "Epoch 34/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0571 - val_loss: 0.0496\n",
      "Epoch 35/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0571 - val_loss: 0.0496\n",
      "Epoch 36/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0571 - val_loss: 0.0496\n",
      "Epoch 37/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0571 - val_loss: 0.0496\n",
      "Epoch 38/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 39/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 40/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 41/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 42/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 43/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 44/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 45/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 46/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 47/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 48/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0571 - val_loss: 0.0494\n",
      "Epoch 49/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0571 - val_loss: 0.0494\n",
      "Epoch 50/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0571 - val_loss: 0.0494\n",
      "Epoch 51/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0571 - val_loss: 0.0494\n",
      "Epoch 52/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0571 - val_loss: 0.0494\n",
      "Epoch 53/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 54/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 55/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 56/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 57/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 58/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 59/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 60/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 61/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 62/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 63/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 64/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 65/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 66/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 67/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 68/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 69/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 70/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 71/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 72/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 73/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 74/100\n",
      "2812/2812 [==============================] - 0s 31us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 75/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 76/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 77/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 78/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 79/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 80/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 81/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 82/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Epoch 83/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Epoch 84/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Epoch 85/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Epoch 86/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Epoch 87/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Epoch 88/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Epoch 89/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Epoch 90/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Epoch 91/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Epoch 92/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Epoch 94/100\n",
      "2812/2812 [==============================] - 0s 40us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Epoch 95/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Epoch 96/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Epoch 97/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Epoch 98/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Epoch 99/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Epoch 100/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Train on 2812 samples, validate on 938 samples\n",
      "Epoch 1/100\n",
      "2812/2812 [==============================] - 0s 120us/sample - loss: 0.1305 - val_loss: 0.1122\n",
      "Epoch 2/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.1228 - val_loss: 0.1053\n",
      "Epoch 3/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.1159 - val_loss: 0.0991\n",
      "Epoch 4/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.1098 - val_loss: 0.0936\n",
      "Epoch 5/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.1042 - val_loss: 0.0887\n",
      "Epoch 6/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0993 - val_loss: 0.0843\n",
      "Epoch 7/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0949 - val_loss: 0.0804\n",
      "Epoch 8/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0909 - val_loss: 0.0769\n",
      "Epoch 9/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0874 - val_loss: 0.0738\n",
      "Epoch 10/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0842 - val_loss: 0.0710\n",
      "Epoch 11/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0814 - val_loss: 0.0685\n",
      "Epoch 12/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0788 - val_loss: 0.0663\n",
      "Epoch 13/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0765 - val_loss: 0.0644\n",
      "Epoch 14/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0745 - val_loss: 0.0626\n",
      "Epoch 15/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0727 - val_loss: 0.0611\n",
      "Epoch 16/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0710 - val_loss: 0.0597\n",
      "Epoch 17/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0696 - val_loss: 0.0585\n",
      "Epoch 18/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0683 - val_loss: 0.0574\n",
      "Epoch 19/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0671 - val_loss: 0.0564\n",
      "Epoch 20/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0660 - val_loss: 0.0555\n",
      "Epoch 21/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0651 - val_loss: 0.0548\n",
      "Epoch 22/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0643 - val_loss: 0.0541\n",
      "Epoch 23/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0635 - val_loss: 0.0535\n",
      "Epoch 24/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0628 - val_loss: 0.0530\n",
      "Epoch 25/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0622 - val_loss: 0.0525\n",
      "Epoch 26/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0617 - val_loss: 0.0521\n",
      "Epoch 27/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0612 - val_loss: 0.0517\n",
      "Epoch 28/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0608 - val_loss: 0.0514\n",
      "Epoch 29/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0604 - val_loss: 0.0511\n",
      "Epoch 30/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0600 - val_loss: 0.0508\n",
      "Epoch 31/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0597 - val_loss: 0.0506\n",
      "Epoch 32/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0594 - val_loss: 0.0504\n",
      "Epoch 33/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0592 - val_loss: 0.0502\n",
      "Epoch 34/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0590 - val_loss: 0.0501\n",
      "Epoch 35/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0588 - val_loss: 0.0500\n",
      "Epoch 36/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0586 - val_loss: 0.0498\n",
      "Epoch 37/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0584 - val_loss: 0.0497\n",
      "Epoch 38/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0583 - val_loss: 0.0497\n",
      "Epoch 39/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0581 - val_loss: 0.0496\n",
      "Epoch 40/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0580 - val_loss: 0.0495\n",
      "Epoch 41/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0579 - val_loss: 0.0495\n",
      "Epoch 42/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0578 - val_loss: 0.0494\n",
      "Epoch 43/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0578 - val_loss: 0.0494\n",
      "Epoch 44/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0577 - val_loss: 0.0493\n",
      "Epoch 45/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0576 - val_loss: 0.0493\n",
      "Epoch 46/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0576 - val_loss: 0.0493\n",
      "Epoch 47/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0575 - val_loss: 0.0493\n",
      "Epoch 48/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0575 - val_loss: 0.0492\n",
      "Epoch 49/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0574 - val_loss: 0.0492\n",
      "Epoch 50/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0574 - val_loss: 0.0492\n",
      "Epoch 51/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0573 - val_loss: 0.0492\n",
      "Epoch 52/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0573 - val_loss: 0.0492\n",
      "Epoch 53/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0573 - val_loss: 0.0492\n",
      "Epoch 54/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0573 - val_loss: 0.0492\n",
      "Epoch 55/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 56/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 57/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 58/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 59/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 60/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 61/100\n",
      "2812/2812 [==============================] - 0s 32us/sample - loss: 0.0571 - val_loss: 0.0492\n",
      "Train on 2812 samples, validate on 938 samples\n",
      "Epoch 1/100\n",
      "2812/2812 [==============================] - 0s 121us/sample - loss: 0.4081 - val_loss: 0.3667\n",
      "Epoch 2/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.3713 - val_loss: 0.3327\n",
      "Epoch 3/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.3384 - val_loss: 0.3024\n",
      "Epoch 4/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.3089 - val_loss: 0.2752\n",
      "Epoch 5/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.2825 - val_loss: 0.2509\n",
      "Epoch 6/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.2589 - val_loss: 0.2292\n",
      "Epoch 7/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.2378 - val_loss: 0.2098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.2189 - val_loss: 0.1925\n",
      "Epoch 9/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.2019 - val_loss: 0.1770\n",
      "Epoch 10/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.1867 - val_loss: 0.1632\n",
      "Epoch 11/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.1732 - val_loss: 0.1508\n",
      "Epoch 12/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.1610 - val_loss: 0.1398\n",
      "Epoch 13/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.1501 - val_loss: 0.1299\n",
      "Epoch 14/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.1404 - val_loss: 0.1211\n",
      "Epoch 15/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.1316 - val_loss: 0.1132\n",
      "Epoch 16/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.1238 - val_loss: 0.1062\n",
      "Epoch 17/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.1168 - val_loss: 0.0999\n",
      "Epoch 18/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.1106 - val_loss: 0.0943\n",
      "Epoch 19/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.1050 - val_loss: 0.0893\n",
      "Epoch 20/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0999 - val_loss: 0.0849\n",
      "Epoch 21/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0954 - val_loss: 0.0809\n",
      "Epoch 22/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0914 - val_loss: 0.0773\n",
      "Epoch 23/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0878 - val_loss: 0.0742\n",
      "Epoch 24/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0846 - val_loss: 0.0713\n",
      "Epoch 25/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0817 - val_loss: 0.0688\n",
      "Epoch 26/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0791 - val_loss: 0.0666\n",
      "Epoch 27/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0768 - val_loss: 0.0646\n",
      "Epoch 28/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0747 - val_loss: 0.0628\n",
      "Epoch 29/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0729 - val_loss: 0.0612\n",
      "Epoch 30/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0712 - val_loss: 0.0598\n",
      "Epoch 31/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0697 - val_loss: 0.0586\n",
      "Epoch 32/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0684 - val_loss: 0.0575\n",
      "Epoch 33/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0672 - val_loss: 0.0565\n",
      "Epoch 34/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0661 - val_loss: 0.0556\n",
      "Epoch 35/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0652 - val_loss: 0.0548\n",
      "Epoch 36/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0643 - val_loss: 0.0542\n",
      "Epoch 37/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0636 - val_loss: 0.0535\n",
      "Epoch 38/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0629 - val_loss: 0.0530\n",
      "Epoch 39/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0623 - val_loss: 0.0525\n",
      "Epoch 40/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0617 - val_loss: 0.0521\n",
      "Epoch 41/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0612 - val_loss: 0.0517\n",
      "Epoch 42/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0608 - val_loss: 0.0514\n",
      "Epoch 43/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0604 - val_loss: 0.0511\n",
      "Epoch 44/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0600 - val_loss: 0.0508\n",
      "Epoch 45/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0597 - val_loss: 0.0506\n",
      "Epoch 46/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0594 - val_loss: 0.0504\n",
      "Epoch 47/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0592 - val_loss: 0.0502\n",
      "Epoch 48/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0590 - val_loss: 0.0501\n",
      "Epoch 49/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0588 - val_loss: 0.0500\n",
      "Epoch 50/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0586 - val_loss: 0.0498\n",
      "Epoch 51/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0584 - val_loss: 0.0497\n",
      "Epoch 52/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0583 - val_loss: 0.0496\n",
      "Epoch 53/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0581 - val_loss: 0.0496\n",
      "Epoch 54/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0580 - val_loss: 0.0495\n",
      "Epoch 55/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0579 - val_loss: 0.0494\n",
      "Epoch 56/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0578 - val_loss: 0.0494\n",
      "Epoch 57/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0577 - val_loss: 0.0493\n",
      "Epoch 58/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0577 - val_loss: 0.0493\n",
      "Epoch 59/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0576 - val_loss: 0.0493\n",
      "Epoch 60/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0575 - val_loss: 0.0493\n",
      "Epoch 61/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0575 - val_loss: 0.0492\n",
      "Epoch 62/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0574 - val_loss: 0.0492\n",
      "Epoch 63/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0574 - val_loss: 0.0492\n",
      "Epoch 64/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0574 - val_loss: 0.0492\n",
      "Epoch 65/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0573 - val_loss: 0.0492\n",
      "Epoch 66/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0573 - val_loss: 0.0492\n",
      "Epoch 67/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0573 - val_loss: 0.0492\n",
      "Epoch 68/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 69/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 70/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 71/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 72/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 73/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0571 - val_loss: 0.0492\n",
      "Epoch 74/100\n",
      "2812/2812 [==============================] - 0s 33us/sample - loss: 0.0571 - val_loss: 0.0492\n",
      "Epoch 75/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0571 - val_loss: 0.0492\n",
      "Train on 2812 samples, validate on 938 samples\n",
      "Epoch 1/100\n",
      "2812/2812 [==============================] - 0s 129us/sample - loss: 1.6270 - val_loss: 1.4954\n",
      "Epoch 2/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 1.4493 - val_loss: 1.3301\n",
      "Epoch 3/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 1.2919 - val_loss: 1.1838\n",
      "Epoch 4/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 1.1524 - val_loss: 1.0542\n",
      "Epoch 5/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 1.0287 - val_loss: 0.9395\n",
      "Epoch 6/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.9191 - val_loss: 0.8378\n",
      "Epoch 7/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.8220 - val_loss: 0.7477\n",
      "Epoch 8/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.7358 - val_loss: 0.6679\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.6594 - val_loss: 0.5972\n",
      "Epoch 10/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.5916 - val_loss: 0.5345\n",
      "Epoch 11/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.5314 - val_loss: 0.4790\n",
      "Epoch 12/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.4781 - val_loss: 0.4298\n",
      "Epoch 13/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.4308 - val_loss: 0.3862\n",
      "Epoch 14/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.3888 - val_loss: 0.3475\n",
      "Epoch 15/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.3515 - val_loss: 0.3132\n",
      "Epoch 16/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.3184 - val_loss: 0.2829\n",
      "Epoch 17/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.2891 - val_loss: 0.2560\n",
      "Epoch 18/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.2630 - val_loss: 0.2322\n",
      "Epoch 19/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.2399 - val_loss: 0.2110\n",
      "Epoch 20/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.2194 - val_loss: 0.1923\n",
      "Epoch 21/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.2012 - val_loss: 0.1757\n",
      "Epoch 22/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.1850 - val_loss: 0.1611\n",
      "Epoch 23/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.1706 - val_loss: 0.1480\n",
      "Epoch 24/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.1579 - val_loss: 0.1365\n",
      "Epoch 25/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.1466 - val_loss: 0.1263\n",
      "Epoch 26/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.1365 - val_loss: 0.1173\n",
      "Epoch 27/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.1276 - val_loss: 0.1093\n",
      "Epoch 28/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.1197 - val_loss: 0.1022\n",
      "Epoch 29/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.1127 - val_loss: 0.0960\n",
      "Epoch 30/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.1064 - val_loss: 0.0904\n",
      "Epoch 31/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.1009 - val_loss: 0.0855\n",
      "Epoch 32/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0960 - val_loss: 0.0812\n",
      "Epoch 33/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0916 - val_loss: 0.0773\n",
      "Epoch 34/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0877 - val_loss: 0.0740\n",
      "Epoch 35/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0843 - val_loss: 0.0710\n",
      "Epoch 36/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0812 - val_loss: 0.0683\n",
      "Epoch 37/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0785 - val_loss: 0.0660\n",
      "Epoch 38/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0761 - val_loss: 0.0639\n",
      "Epoch 39/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0739 - val_loss: 0.0621\n",
      "Epoch 40/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0721 - val_loss: 0.0605\n",
      "Epoch 41/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0704 - val_loss: 0.0591\n",
      "Epoch 42/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0689 - val_loss: 0.0578\n",
      "Epoch 43/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0675 - val_loss: 0.0567\n",
      "Epoch 44/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0664 - val_loss: 0.0558\n",
      "Epoch 45/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0653 - val_loss: 0.0549\n",
      "Epoch 46/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0644 - val_loss: 0.0542\n",
      "Epoch 47/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0636 - val_loss: 0.0535\n",
      "Epoch 48/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0628 - val_loss: 0.0529\n",
      "Epoch 49/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0622 - val_loss: 0.0524\n",
      "Epoch 50/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0616 - val_loss: 0.0520\n",
      "Epoch 51/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0611 - val_loss: 0.0516\n",
      "Epoch 52/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0606 - val_loss: 0.0513\n",
      "Epoch 53/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0602 - val_loss: 0.0510\n",
      "Epoch 54/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0599 - val_loss: 0.0507\n",
      "Epoch 55/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0595 - val_loss: 0.0505\n",
      "Epoch 56/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0593 - val_loss: 0.0503\n",
      "Epoch 57/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0590 - val_loss: 0.0501\n",
      "Epoch 58/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0588 - val_loss: 0.0500\n",
      "Epoch 59/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0586 - val_loss: 0.0498\n",
      "Epoch 60/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0584 - val_loss: 0.0497\n",
      "Epoch 61/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0583 - val_loss: 0.0496\n",
      "Epoch 62/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0581 - val_loss: 0.0496\n",
      "Epoch 63/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0580 - val_loss: 0.0495\n",
      "Epoch 64/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0579 - val_loss: 0.0494\n",
      "Epoch 65/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0578 - val_loss: 0.0494\n",
      "Epoch 66/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0577 - val_loss: 0.0493\n",
      "Epoch 67/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0576 - val_loss: 0.0493\n",
      "Epoch 68/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0576 - val_loss: 0.0493\n",
      "Epoch 69/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0575 - val_loss: 0.0492\n",
      "Epoch 70/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0574 - val_loss: 0.0492\n",
      "Epoch 71/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0574 - val_loss: 0.0492\n",
      "Epoch 72/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0574 - val_loss: 0.0492\n",
      "Epoch 73/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0573 - val_loss: 0.0492\n",
      "Epoch 74/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0573 - val_loss: 0.0492\n",
      "Epoch 75/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0573 - val_loss: 0.0492\n",
      "Epoch 76/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 77/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 78/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 79/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 80/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 81/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0571 - val_loss: 0.0492\n",
      "Epoch 82/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0571 - val_loss: 0.0492\n",
      "Epoch 83/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0571 - val_loss: 0.0492\n",
      "Train on 2812 samples, validate on 938 samples\n",
      "Epoch 1/100\n",
      "2812/2812 [==============================] - 0s 134us/sample - loss: 0.0952 - val_loss: 0.0806\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0911 - val_loss: 0.0770\n",
      "Epoch 3/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0874 - val_loss: 0.0737\n",
      "Epoch 4/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0841 - val_loss: 0.0708\n",
      "Epoch 5/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0811 - val_loss: 0.0683\n",
      "Epoch 6/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0785 - val_loss: 0.0660\n",
      "Epoch 7/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0762 - val_loss: 0.0640\n",
      "Epoch 8/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0741 - val_loss: 0.0622\n",
      "Epoch 9/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0722 - val_loss: 0.0607\n",
      "Epoch 10/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0706 - val_loss: 0.0593\n",
      "Epoch 11/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0691 - val_loss: 0.0580\n",
      "Epoch 12/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0678 - val_loss: 0.0570\n",
      "Epoch 13/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0666 - val_loss: 0.0560\n",
      "Epoch 14/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0656 - val_loss: 0.0551\n",
      "Epoch 15/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0646 - val_loss: 0.0544\n",
      "Epoch 16/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0638 - val_loss: 0.0537\n",
      "Epoch 17/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0631 - val_loss: 0.0531\n",
      "Epoch 18/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0624 - val_loss: 0.0526\n",
      "Epoch 19/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0618 - val_loss: 0.0522\n",
      "Epoch 20/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0613 - val_loss: 0.0518\n",
      "Epoch 21/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0608 - val_loss: 0.0514\n",
      "Epoch 22/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0604 - val_loss: 0.0511\n",
      "Epoch 23/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0601 - val_loss: 0.0508\n",
      "Epoch 24/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0597 - val_loss: 0.0506\n",
      "Epoch 25/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0594 - val_loss: 0.0504\n",
      "Epoch 26/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0592 - val_loss: 0.0502\n",
      "Epoch 27/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0589 - val_loss: 0.0501\n",
      "Epoch 28/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0587 - val_loss: 0.0499\n",
      "Epoch 29/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0585 - val_loss: 0.0498\n",
      "Epoch 30/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0584 - val_loss: 0.0497\n",
      "Epoch 31/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0582 - val_loss: 0.0496\n",
      "Epoch 32/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0581 - val_loss: 0.0495\n",
      "Epoch 33/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0580 - val_loss: 0.0495\n",
      "Epoch 34/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0579 - val_loss: 0.0494\n",
      "Epoch 35/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0578 - val_loss: 0.0494\n",
      "Epoch 36/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0577 - val_loss: 0.0493\n",
      "Epoch 37/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0576 - val_loss: 0.0493\n",
      "Epoch 38/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0576 - val_loss: 0.0493\n",
      "Epoch 39/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0575 - val_loss: 0.0492\n",
      "Epoch 40/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0575 - val_loss: 0.0492\n",
      "Epoch 41/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0574 - val_loss: 0.0492\n",
      "Epoch 42/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0574 - val_loss: 0.0492\n",
      "Epoch 43/100\n",
      "2812/2812 [==============================] - 0s 34us/sample - loss: 0.0573 - val_loss: 0.0492\n",
      "Epoch 44/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0573 - val_loss: 0.0492\n",
      "Epoch 45/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0573 - val_loss: 0.0492\n",
      "Epoch 46/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 47/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 48/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 49/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 50/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0572 - val_loss: 0.0492\n",
      "Epoch 51/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0571 - val_loss: 0.0492\n",
      "Epoch 52/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0571 - val_loss: 0.0492\n",
      "Epoch 53/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0571 - val_loss: 0.0492\n",
      "Train on 2812 samples, validate on 938 samples\n",
      "Epoch 1/100\n",
      "2812/2812 [==============================] - 0s 144us/sample - loss: 0.1063 - val_loss: 0.1013\n",
      "Epoch 2/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.1012 - val_loss: 0.0963\n",
      "Epoch 3/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0967 - val_loss: 0.0917\n",
      "Epoch 4/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0926 - val_loss: 0.0876\n",
      "Epoch 5/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0890 - val_loss: 0.0839\n",
      "Epoch 6/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0857 - val_loss: 0.0806\n",
      "Epoch 7/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0828 - val_loss: 0.0776\n",
      "Epoch 8/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0802 - val_loss: 0.0749\n",
      "Epoch 9/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0778 - val_loss: 0.0725\n",
      "Epoch 10/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0757 - val_loss: 0.0703\n",
      "Epoch 11/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0738 - val_loss: 0.0683\n",
      "Epoch 12/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0721 - val_loss: 0.0665\n",
      "Epoch 13/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0705 - val_loss: 0.0649\n",
      "Epoch 14/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0691 - val_loss: 0.0635\n",
      "Epoch 15/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0679 - val_loss: 0.0622\n",
      "Epoch 16/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0668 - val_loss: 0.0610\n",
      "Epoch 17/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0658 - val_loss: 0.0599\n",
      "Epoch 18/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0649 - val_loss: 0.0589\n",
      "Epoch 19/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0641 - val_loss: 0.0581\n",
      "Epoch 20/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0634 - val_loss: 0.0573\n",
      "Epoch 21/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0627 - val_loss: 0.0565\n",
      "Epoch 22/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0621 - val_loss: 0.0559\n",
      "Epoch 23/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0616 - val_loss: 0.0553\n",
      "Epoch 24/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0611 - val_loss: 0.0548\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0607 - val_loss: 0.0543\n",
      "Epoch 26/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0603 - val_loss: 0.0539\n",
      "Epoch 27/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0600 - val_loss: 0.0535\n",
      "Epoch 28/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0597 - val_loss: 0.0531\n",
      "Epoch 29/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0594 - val_loss: 0.0528\n",
      "Epoch 30/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0592 - val_loss: 0.0525\n",
      "Epoch 31/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0590 - val_loss: 0.0522\n",
      "Epoch 32/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0588 - val_loss: 0.0520\n",
      "Epoch 33/100\n",
      "2812/2812 [==============================] - 0s 38us/sample - loss: 0.0586 - val_loss: 0.0518\n",
      "Epoch 34/100\n",
      "2812/2812 [==============================] - 0s 38us/sample - loss: 0.0584 - val_loss: 0.0516\n",
      "Epoch 35/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0583 - val_loss: 0.0514\n",
      "Epoch 36/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0582 - val_loss: 0.0512\n",
      "Epoch 37/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0580 - val_loss: 0.0511\n",
      "Epoch 38/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0579 - val_loss: 0.0509\n",
      "Epoch 39/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0578 - val_loss: 0.0508\n",
      "Epoch 40/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0578 - val_loss: 0.0507\n",
      "Epoch 41/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0577 - val_loss: 0.0506\n",
      "Epoch 42/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0576 - val_loss: 0.0505\n",
      "Epoch 43/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0576 - val_loss: 0.0504\n",
      "Epoch 44/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0575 - val_loss: 0.0503\n",
      "Epoch 45/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0575 - val_loss: 0.0502\n",
      "Epoch 46/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0574 - val_loss: 0.0502\n",
      "Epoch 47/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0574 - val_loss: 0.0501\n",
      "Epoch 48/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0573 - val_loss: 0.0500\n",
      "Epoch 49/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0573 - val_loss: 0.0500\n",
      "Epoch 50/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0573 - val_loss: 0.0499\n",
      "Epoch 51/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0572 - val_loss: 0.0499\n",
      "Epoch 52/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0572 - val_loss: 0.0499\n",
      "Epoch 53/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0572 - val_loss: 0.0498\n",
      "Epoch 54/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0572 - val_loss: 0.0498\n",
      "Epoch 55/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0572 - val_loss: 0.0498\n",
      "Epoch 56/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0572 - val_loss: 0.0497\n",
      "Epoch 57/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0571 - val_loss: 0.0497\n",
      "Epoch 58/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0571 - val_loss: 0.0497\n",
      "Epoch 59/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0571 - val_loss: 0.0496\n",
      "Epoch 60/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0571 - val_loss: 0.0496\n",
      "Epoch 61/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0571 - val_loss: 0.0496\n",
      "Epoch 62/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0571 - val_loss: 0.0496\n",
      "Epoch 63/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0571 - val_loss: 0.0496\n",
      "Epoch 64/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0571 - val_loss: 0.0496\n",
      "Epoch 65/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 66/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 67/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 68/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 69/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 70/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 71/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0495\n",
      "Epoch 72/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0570 - val_loss: 0.0495\n",
      "Epoch 73/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0495\n",
      "Epoch 74/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 75/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 76/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 77/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 78/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 79/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 80/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 81/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 82/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 83/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 84/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 85/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 86/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 87/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 88/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 89/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 90/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 91/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 92/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 93/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 94/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 95/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 96/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 97/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 98/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 99/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 100/100\n",
      "2812/2812 [==============================] - 0s 35us/sample - loss: 0.0570 - val_loss: 0.0493\n",
      "Train on 2812 samples, validate on 938 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2812/2812 [==============================] - 0s 151us/sample - loss: 0.2964 - val_loss: 0.2877\n",
      "Epoch 2/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.2718 - val_loss: 0.2639\n",
      "Epoch 3/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.2498 - val_loss: 0.2425\n",
      "Epoch 4/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.2301 - val_loss: 0.2232\n",
      "Epoch 5/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.2124 - val_loss: 0.2059\n",
      "Epoch 6/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.1965 - val_loss: 0.1904\n",
      "Epoch 7/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.1822 - val_loss: 0.1764\n",
      "Epoch 8/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.1694 - val_loss: 0.1638\n",
      "Epoch 9/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.1579 - val_loss: 0.1525\n",
      "Epoch 10/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.1476 - val_loss: 0.1423\n",
      "Epoch 11/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.1383 - val_loss: 0.1332\n",
      "Epoch 12/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.1300 - val_loss: 0.1249\n",
      "Epoch 13/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.1225 - val_loss: 0.1175\n",
      "Epoch 14/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.1158 - val_loss: 0.1109\n",
      "Epoch 15/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.1098 - val_loss: 0.1049\n",
      "Epoch 16/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.1044 - val_loss: 0.0995\n",
      "Epoch 17/100\n",
      "2812/2812 [==============================] - 0s 38us/sample - loss: 0.0996 - val_loss: 0.0946\n",
      "Epoch 18/100\n",
      "2812/2812 [==============================] - 0s 40us/sample - loss: 0.0952 - val_loss: 0.0902\n",
      "Epoch 19/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0913 - val_loss: 0.0863\n",
      "Epoch 20/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0878 - val_loss: 0.0827\n",
      "Epoch 21/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0847 - val_loss: 0.0795\n",
      "Epoch 22/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0818 - val_loss: 0.0766\n",
      "Epoch 23/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0793 - val_loss: 0.0740\n",
      "Epoch 24/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0770 - val_loss: 0.0717\n",
      "Epoch 25/100\n",
      "2812/2812 [==============================] - 0s 38us/sample - loss: 0.0750 - val_loss: 0.0696\n",
      "Epoch 26/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0731 - val_loss: 0.0677\n",
      "Epoch 27/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0715 - val_loss: 0.0659\n",
      "Epoch 28/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0700 - val_loss: 0.0644\n",
      "Epoch 29/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0687 - val_loss: 0.0630\n",
      "Epoch 30/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0675 - val_loss: 0.0617\n",
      "Epoch 31/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0664 - val_loss: 0.0606\n",
      "Epoch 32/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0655 - val_loss: 0.0595\n",
      "Epoch 33/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0646 - val_loss: 0.0586\n",
      "Epoch 34/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0638 - val_loss: 0.0578\n",
      "Epoch 35/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0631 - val_loss: 0.0570\n",
      "Epoch 36/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0625 - val_loss: 0.0563\n",
      "Epoch 37/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0620 - val_loss: 0.0557\n",
      "Epoch 38/100\n",
      "2812/2812 [==============================] - 0s 44us/sample - loss: 0.0614 - val_loss: 0.0551\n",
      "Epoch 39/100\n",
      "2812/2812 [==============================] - 0s 40us/sample - loss: 0.0610 - val_loss: 0.0546\n",
      "Epoch 40/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0606 - val_loss: 0.0541\n",
      "Epoch 41/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0602 - val_loss: 0.0537\n",
      "Epoch 42/100\n",
      "2812/2812 [==============================] - 0s 38us/sample - loss: 0.0599 - val_loss: 0.0533\n",
      "Epoch 43/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0596 - val_loss: 0.0530\n",
      "Epoch 44/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0593 - val_loss: 0.0527\n",
      "Epoch 45/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0591 - val_loss: 0.0524\n",
      "Epoch 46/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0589 - val_loss: 0.0521\n",
      "Epoch 47/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0587 - val_loss: 0.0519\n",
      "Epoch 48/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0585 - val_loss: 0.0517\n",
      "Epoch 49/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0584 - val_loss: 0.0515\n",
      "Epoch 50/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0582 - val_loss: 0.0513\n",
      "Epoch 51/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0581 - val_loss: 0.0512\n",
      "Epoch 52/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0580 - val_loss: 0.0510\n",
      "Epoch 53/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0579 - val_loss: 0.0509\n",
      "Epoch 54/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0578 - val_loss: 0.0508\n",
      "Epoch 55/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0577 - val_loss: 0.0506\n",
      "Epoch 56/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0577 - val_loss: 0.0505\n",
      "Epoch 57/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0576 - val_loss: 0.0504\n",
      "Epoch 58/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0575 - val_loss: 0.0504\n",
      "Epoch 59/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0575 - val_loss: 0.0503\n",
      "Epoch 60/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0574 - val_loss: 0.0502\n",
      "Epoch 61/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0574 - val_loss: 0.0501\n",
      "Epoch 62/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0574 - val_loss: 0.0501\n",
      "Epoch 63/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0573 - val_loss: 0.0500\n",
      "Epoch 64/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0573 - val_loss: 0.0500\n",
      "Epoch 65/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0573 - val_loss: 0.0499\n",
      "Epoch 66/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0572 - val_loss: 0.0499\n",
      "Epoch 67/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0572 - val_loss: 0.0498\n",
      "Epoch 68/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0572 - val_loss: 0.0498\n",
      "Epoch 69/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0572 - val_loss: 0.0498\n",
      "Epoch 70/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0572 - val_loss: 0.0497\n",
      "Epoch 71/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0571 - val_loss: 0.0497\n",
      "Epoch 72/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0571 - val_loss: 0.0497\n",
      "Epoch 73/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0571 - val_loss: 0.0497\n",
      "Epoch 74/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0571 - val_loss: 0.0496\n",
      "Epoch 75/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0571 - val_loss: 0.0496\n",
      "Epoch 76/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0571 - val_loss: 0.0496\n",
      "Epoch 77/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0571 - val_loss: 0.0496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0571 - val_loss: 0.0496\n",
      "Epoch 79/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0571 - val_loss: 0.0496\n",
      "Epoch 80/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 81/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 82/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 83/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 84/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0571 - val_loss: 0.0495\n",
      "Epoch 85/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0570 - val_loss: 0.0495\n",
      "Epoch 86/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0570 - val_loss: 0.0495\n",
      "Epoch 87/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0495\n",
      "Epoch 88/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0570 - val_loss: 0.0495\n",
      "Epoch 89/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 90/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 91/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 92/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 93/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 94/100\n",
      "2812/2812 [==============================] - 0s 36us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 95/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 96/100\n",
      "2812/2812 [==============================] - 0s 37us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 97/100\n",
      "2812/2812 [==============================] - 0s 38us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 98/100\n",
      "2812/2812 [==============================] - 0s 38us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 99/100\n",
      "2812/2812 [==============================] - 0s 39us/sample - loss: 0.0570 - val_loss: 0.0494\n",
      "Epoch 100/100\n",
      "2812/2812 [==============================] - 0s 39us/sample - loss: 0.0570 - val_loss: 0.0494\n"
     ]
    }
   ],
   "source": [
    "for hidden_layers in layers:\n",
    "    model = build_model(hidden_layers = hidden_layers)\n",
    "    history = model.fit(x_train, y_train, epochs = 100,\n",
    "                        validation_data = (x_valid, y_valid),\n",
    "                        callbacks = callbacks)\n",
    "    from tensorflow import float64\n",
    "    # get weights and bias in each layer\n",
    "    i = 0\n",
    "    kernel_tensor = []\n",
    "    bias_tensor = []\n",
    "    for each in model.weights:\n",
    "        if i % 2 is 0:\n",
    "            kernel_tensor.append(tf.constant(each.numpy(), dtype=float64))\n",
    "        else:\n",
    "            bias_tensor.append(tf.constant(each.numpy(), dtype=float64))\n",
    "        i += 1\n",
    "    # first derivatives\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        output = s(input_x, input_y, input_z, input_t)\n",
    "        \n",
    "    ds_x = tape.gradient(output, input_x).numpy()\n",
    "    ds_y = tape.gradient(output, input_y).numpy()\n",
    "    ds_z = tape.gradient(output, input_z).numpy()\n",
    "    ds_t = tape.gradient(output, input_t).numpy()\n",
    "\n",
    "    del tape\n",
    "    dx.append((ds_x.reshape(len(ds_x))-u_x).sum()/len(ds_x))\n",
    "    dy.append((ds_y.reshape(len(ds_y))-u_y).sum()/len(ds_y))\n",
    "    dz.append((ds_z.reshape(len(ds_z))-u_z).sum()/len(ds_z))\n",
    "    dt.append((ds_t.reshape(len(ds_t))-u_t).sum()/len(ds_t))\n",
    "    # second derivatives\n",
    "    with tf.GradientTape(persistent=True) as outer_tape:\n",
    "        with tf.GradientTape(persistent=True) as inner_tape:\n",
    "            output = s(input_x, input_y, input_z, input_t)\n",
    "        inner_grads = inner_tape.gradient(output, [input_x, input_y, input_z])\n",
    "    # outer_grads = [outer_tape.gradient(inner_grad, [input_x, input_y, input_z]) for inner_grad in inner_grads]\n",
    "    ds_xx = outer_tape.gradient(inner_grads[0], [input_x])[0].numpy()\n",
    "    ds_yy = outer_tape.gradient(inner_grads[1], [input_y])[0].numpy()\n",
    "    ds_zz = outer_tape.gradient(inner_grads[2], [input_z])[0].numpy()\n",
    "\n",
    "    del inner_tape\n",
    "    del outer_tape\n",
    "    dxx.append((ds_xx.reshape(len(ds_xx))-u_xx).sum()/len(ds_xx))\n",
    "    dyy.append((ds_yy.reshape(len(ds_yy))-u_yy).sum()/len(ds_yy))\n",
    "    dzz.append((ds_zz.reshape(len(ds_zz))-u_zz).sum()/len(ds_zz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the figure \n",
    "plt.plot(layers, list(map(abs, dx)), label='u_x')\n",
    "plt.plot(layers, list(map(abs, dy)), label='u_y')\n",
    "plt.plot(layers, list(map(abs, dz)), label='u_z')\n",
    "# plt.plot(layers, list(map(abs, dt)), label='u_t')\n",
    "plt.legend()\n",
    "plt.xlabel('layers')\n",
    "plt.ylabel('error(Ave.)')\n",
    "plt.show()\n",
    "plt.plot(layers, dxx, label='u_xx')\n",
    "plt.plot(layers, dyy, label='u_yy')\n",
    "plt.plot(layers, dzz, label='u_zz')\n",
    "plt.legend()\n",
    "plt.xlabel('layers')\n",
    "plt.ylabel('error(Ave.)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_ratio(x, y, z, u0):\n",
    "    du0 = np.array([3, 4, 5])\n",
    "    ratio = ((x ** 2 + y ** 2 + z ** 2) ** 0.5 / \n",
    "             (du0[0] ** 2 + du0[1] ** 2 + du0[2] ** 2) ** 0.5)\n",
    "    return ratio\n",
    "\n",
    "print('Length ratio : {}'.format(length_ratio(ds_x, ds_y, ds_z, input_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def theta(x, y, z, u0):\n",
    "    du0 = np.array([3, 4, 5])\n",
    "    mol_du0 = (du0[0] ** 2 + du0[1] ** 2 + du0[2] ** 2) ** 0.5\n",
    "    mol_xyz = (x ** 2 + y ** 2 + z ** 2) ** 0.5\n",
    "    acos = math.acos(\n",
    "        np.dot(du0, u0[0]) / (mol_du0 * mol_xyz))\n",
    "    return acos\n",
    "\n",
    "print('Theta : {} rad'.format(theta(ds_x, ds_y, ds_z, input_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
